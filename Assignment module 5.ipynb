{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2d3daf",
   "metadata": {},
   "source": [
    "# Assignment module 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c10c6c",
   "metadata": {},
   "source": [
    "This assignment is about clustering. The dataset used contains information about the traffic flow during each 5 minute time interval in the year 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76055925",
   "metadata": {},
   "source": [
    "### Package import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e39631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\n",
    "import sklearn.metrics.pairwise as dis_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6859183",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09c9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"dataset_exercise_5_clustering_highway_traffic.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaebc47",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592c25a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 39.  18.  26. ...  32.  39.  34.]\n",
      " [ 30.  32.  27. ...  44.  41.  39.]\n",
      " [ 36.  44.  52. ...  50.  45.  23.]\n",
      " ...\n",
      " [ 20.  34.  31. ...  38.  42.  36.]\n",
      " [ 36.  40.  25. ...  38.  56.  35.]\n",
      " [ 33.  32.  34. ... 130. 129. 117.]]\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame 'data_df' by columns \"Date\" and \"Interval_5\"\n",
    "data_df.sort_values([\"Date\", \"Interval_5\"])\n",
    "\n",
    "#print(data_df)\n",
    "\n",
    "# Extract unique dates from the sorted DataFrame\n",
    "days = np.unique(data_df[['Date']].values.ravel())\n",
    "#print(days)\n",
    "\n",
    "# Calculate the total number of unique days\n",
    "ndays = len(days)\n",
    "\n",
    "# Group the DataFrame 'data_df' by the \"Date\" column\n",
    "day_subsets_df = data_df.groupby([\"Date\"])\n",
    "\n",
    "# Define the total number of 5-minute intervals in a day\n",
    "nintvals = 288\n",
    "\n",
    "# Create a matrix 'vectorized_day_dataset' filled with NaN values\n",
    "vectorized_day_dataset = np.zeros((ndays, nintvals))\n",
    "vectorized_day_dataset.fill(np.nan)\n",
    "\n",
    "# Loop through each unique day\n",
    "for i in range(0, ndays):\n",
    "    # Get the DataFrame corresponding to the current day\n",
    "    df_t = day_subsets_df.get_group(days[i])\n",
    "\n",
    "    # Loop through each row in the current day's DataFrame\n",
    "    for j in range(len(df_t)):\n",
    "        # Get the current day's DataFrame\n",
    "        df_t = day_subsets_df.get_group(days[i])\n",
    "\n",
    "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset'\n",
    "        vectorized_day_dataset[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
    "\n",
    "# Print the resulting 'vectorized_day_dataset'\n",
    "print(vectorized_day_dataset)\n",
    "\n",
    "\n",
    "# Create an array 'day_of_week' to store the day of the week for each unique date\n",
    "day_of_week = np.zeros((ndays))\n",
    "\n",
    "# Loop through each unique date\n",
    "for i in range(0, ndays):\n",
    "    # Parse the current date from a string to a datetime object\n",
    "    day_dt = datetime.datetime.strptime(str(days[i]), '%Y%m%d')\n",
    "\n",
    "    # Get the day of the week (1 for Monday, 2 for Tuesday, ..., 7 for Sunday)\n",
    "    day_of_week[i] = day_dt.isoweekday()\n",
    "    \n",
    "nans_per_day = np.sum(np.isnan(vectorized_day_dataset),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b7de1",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57444a57",
   "metadata": {},
   "source": [
    "### Internal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc6c714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Clusters    KMeans  AgglomerativeClustering    DBSCAN  GaussianMixture\n",
      "0         2  0.307621                 0.290945 -0.027721         0.307557\n",
      "1         3  0.300536                 0.268112 -0.027721         0.279376\n",
      "2         4  0.279691                 0.257637 -0.027721         0.228264\n",
      "3         5  0.223437                 0.263778 -0.027721         0.232653\n",
      "4         6  0.236851                 0.250363 -0.027721         0.246955\n",
      "5         7  0.237882                 0.243157 -0.027721         0.220194\n",
      "6         8  0.232969                 0.239635 -0.027721         0.220029\n",
      "7         9  0.223869                 0.233039 -0.027721         0.182827\n",
      "8        10  0.207933                 0.215062 -0.027721         0.176048\n",
      "   Clusters    KMeans  AgglomerativeClustering    DBSCAN  GaussianMixture\n",
      "0         2  1.267725                 1.279138  2.379572         1.269963\n",
      "1         3  1.192975                 1.313811  2.379572         1.319473\n",
      "2         4  1.157241                 1.460274  2.379572         1.789181\n",
      "3         5  1.495123                 1.231404  2.379572         1.695350\n",
      "4         6  1.452224                 1.460171  2.379572         1.486734\n",
      "5         7  1.426838                 1.513606  2.379572         1.546144\n",
      "6         8  1.455358                 1.440534  2.379572         1.417611\n",
      "7         9  1.530087                 1.390687  2.379572         1.524012\n",
      "8        10  1.610391                 1.415068  2.379572         1.675019\n",
      "   Clusters      KMeans  AgglomerativeClustering     DBSCAN  GaussianMixture\n",
      "0         2  174.104884               159.821505  35.327906       174.101733\n",
      "1         3  115.200961               148.607605  35.327906       158.986940\n",
      "2         4  131.987321               127.187152  35.327906       122.675243\n",
      "3         5  114.764242               118.054696  35.327906       114.400558\n",
      "4         6  115.325098               108.679506  35.327906       108.965736\n",
      "5         7   98.112519               100.749684  35.327906       102.991515\n",
      "6         8   90.291238                91.736572  35.327906        94.653013\n",
      "7         9   81.538265                84.412420  35.327906        81.909512\n",
      "8        10   77.646906                78.955196  35.327906        82.334671\n"
     ]
    }
   ],
   "source": [
    "vectorized_day_dataset_no_nans = vectorized_day_dataset[np.where(nans_per_day == 0)[0],:]\n",
    "days_not_nans = days[np.where(nans_per_day == 0)[0]]\n",
    "\n",
    "clusters = None\n",
    "cluster_model_list = [\"KMeans\", \"AgglomerativeClustering\", \"DBSCAN\", \"GaussianMixture\"]\n",
    "cluster_range_list = range(2, 11)\n",
    "\n",
    "df_results_silhouette = pd.DataFrame(data=cluster_range_list, columns = [\"Clusters\"])\n",
    "df_results_davies_bouldin = pd.DataFrame(data=cluster_range_list, columns = [\"Clusters\"])\n",
    "df_results_calinski_harabasz = pd.DataFrame(data=cluster_range_list, columns = [\"Clusters\"])\n",
    "\n",
    "for i in cluster_model_list:\n",
    "    silhouette_list = []\n",
    "    davies_bouldin_list = []\n",
    "    calinski_harabasz_list = []    \n",
    "    for j in cluster_range_list:\n",
    "        n_clusters = j\n",
    "        if i == \"KMeans\":\n",
    "            clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans)\n",
    "            cluster_labels = clusters.labels_\n",
    "        elif i == \"AgglomerativeClustering\":\n",
    "            clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans)\n",
    "            cluster_labels = clusters.labels_\n",
    "        elif i == \"DBSCAN\":\n",
    "            clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans)\n",
    "            cluster_labels = clusters.labels_\n",
    "        elif i == \"GaussianMixture\":\n",
    "            cluster_labels = GaussianMixture(n_components=n_clusters).fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans)\n",
    "        else:\n",
    "            print(\"Error: Incorrect cluster model\")\n",
    "        \n",
    "        # Calculate the number of clusters by finding unique values in 'cluster_labels'\n",
    "        n_clusters_t = len(np.unique(cluster_labels))\n",
    "        \n",
    "        # Calculate the Silhouette Score\n",
    "        SC_score = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
    "        silhouette_list.append(SC_score)\n",
    "        # Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
    "\n",
    "        # Calculate the Davies-Bouldin Score\n",
    "        DB_score = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
    "        davies_bouldin_list.append(DB_score)\n",
    "        # Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
    "\n",
    "        # Calculate the Calinski-Harabasz Score\n",
    "        CH_score = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
    "        calinski_harabasz_list.append(CH_score)\n",
    "        # Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
    "        \n",
    "    df_results_silhouette[i] = silhouette_list\n",
    "    df_results_davies_bouldin[i] = davies_bouldin_list\n",
    "    df_results_calinski_harabasz[i] = calinski_harabasz_list \n",
    "               \n",
    "print(df_results_silhouette) \n",
    "print(df_results_davies_bouldin)\n",
    "print(df_results_calinski_harabasz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bd6c9",
   "metadata": {},
   "source": [
    "### Parameter optimalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff359f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.07376899788914677\n",
      "Davies-Bouldin Score: 2.6764334562222967\n",
      "Calinski-Harabasz Score: 30.0559655749271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomko\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 2\n",
    "\n",
    "#clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\", algorithm=\"elkan\").fit(vectorized_day_dataset_no_nans) # Parameter tested: {“lloyd”, “elkan”, “auto”, “full”}\n",
    "#clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "#clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "#if clusters is not None:\n",
    "#  cluster_labels = clusters.labels_\n",
    "\n",
    "cluster_labels = GaussianMixture(n_components=n_clusters, covariance_type=\"spherical\").fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #covariance_type{‘full’, ‘tied’, ‘diag’, ‘spherical’}\n",
    "\n",
    "\n",
    "# Calculate the Silhouette Score\n",
    "SC_score_improve = silhouette_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
    "# Silhouette Score measures the quality of clusters, higher values indicate better separation.\n",
    "\n",
    "# Calculate the Davies-Bouldin Score\n",
    "DB_score_improve = davies_bouldin_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
    "# Davies-Bouldin Score measures the average similarity between each cluster and its most similar cluster, lower values indicate better separation.\n",
    "\n",
    "# Calculate the Calinski-Harabasz Score\n",
    "CH_score_improve = calinski_harabasz_score(vectorized_day_dataset_no_nans, cluster_labels)\n",
    "# Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance, higher values indicate better separation.\n",
    "\n",
    "# Print the computed cluster quality scores\n",
    "print('Silhouette Score:', SC_score_improve)\n",
    "print('Davies-Bouldin Score:', DB_score_improve)\n",
    "print('Calinski-Harabasz Score:', CH_score_improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e8e9c",
   "metadata": {},
   "source": [
    "### External evaluation with short-term prediction: Import evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b956d674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.74 17.57 16.94 ... 19.43 19.88 19.73]\n",
      " [19.17 19.26 20.98 ... 19.34 21.22 19.79]\n",
      " [19.69 19.39 19.76 ... 20.51 20.5  19.6 ]\n",
      " ...\n",
      " [19.65 22.59 20.8  ... 19.23 20.32 20.18]\n",
      " [19.75 19.96 20.95 ... 17.95 19.14 19.55]\n",
      " [20.82 20.28 20.5  ... 19.72 20.64 20.23]]\n",
      "Number of NaNs: 96\n",
      "Rate of NaNs: 0.004166666666666667\n",
      "Number of days with missing values: 11\n",
      "Final number of days in evaluation dataset: 69\n",
      "List of days without missing values: [20220108 20220109 20220131 20220204 20220209 20220210 20220211 20220223\n",
      " 20220226 20220227 20220302 20220304 20220305 20220306 20220310 20220314\n",
      " 20220315 20220321 20220323 20220326 20220403 20220406 20220416 20220418\n",
      " 20220421 20220422 20220425 20220427 20220428 20220503 20220505 20220514\n",
      " 20220519 20220521 20220522 20220526 20220530 20220601 20220603 20220609\n",
      " 20220616 20220619 20220623 20220628 20220704 20220711 20220712 20220904\n",
      " 20220910 20220911 20220920 20220921 20220925 20220927 20220929 20220930\n",
      " 20221005 20221022 20221024 20221114 20221116 20221121 20221122 20221213\n",
      " 20221216 20221218 20221220 20221223 20221230]\n"
     ]
    }
   ],
   "source": [
    "# Read the evaluation dataset from a CSV file\n",
    "data_eval_df = pd.read_csv(\"evaluation_dataset_exercise_5_clustering_highway_traffic.csv\", sep=\";\")\n",
    "\n",
    "# Sort the evaluation DataFrame by columns \"Date\" and \"Interval_5\"\n",
    "data_eval_df.sort_values([\"Date\", \"Interval_5\"])\n",
    "\n",
    "# Extract unique dates from the sorted evaluation DataFrame\n",
    "days_eval = np.unique(data_eval_df[['Date']].values.ravel())\n",
    "# Calculate the total number of unique days in the evaluation dataset\n",
    "ndays_eval = len(days_eval)\n",
    "\n",
    "# Group the evaluation DataFrame by the \"Date\" column\n",
    "day_eval_subsets_df = data_eval_df.groupby([\"Date\"])\n",
    "\n",
    "# Initialize a matrix 'vectorized_day_dataset_eval' filled with NaN values\n",
    "vectorized_day_dataset_eval = np.zeros((ndays_eval, nintvals))\n",
    "vectorized_day_dataset_eval.fill(np.nan)\n",
    "# This section initializes a 2D array to store the evaluation dataset and fills it with NaN values.\n",
    "\n",
    "# Loop through each unique day in the evaluation dataset\n",
    "for i in range(0, ndays_eval):\n",
    "    # Get the DataFrame corresponding to the current day\n",
    "    df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
    "\n",
    "    # Loop through each row in the current day's DataFrame\n",
    "    for j in range(len(df_t)):\n",
    "        # Get the current day's DataFrame (this line is redundant)\n",
    "        df_t = day_eval_subsets_df.get_group(days_eval[i])\n",
    "\n",
    "        # Extract the \"Interval_5\" and \"flow\" values and populate 'vectorized_day_dataset_eval'\n",
    "        vectorized_day_dataset_eval[i, df_t.iloc[j][\"Interval_5\"]] = df_t.iloc[j][\"flow\"]\n",
    "\n",
    "# Print the resulting 'vectorized_day_dataset_eval'\n",
    "print(vectorized_day_dataset_eval)\n",
    "\n",
    "\n",
    "# Calculate the total number of NaN values in the evaluation dataset\n",
    "print('Number of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)))\n",
    "\n",
    "# Calculate the rate of NaN values in the evaluation dataset\n",
    "print('Rate of NaNs:', np.sum(np.isnan(vectorized_day_dataset_eval)) / (ndays_eval * nintvals))\n",
    "\n",
    "# Calculate the number of days with missing values\n",
    "nans_per_day_eval = np.sum(np.isnan(vectorized_day_dataset_eval), 1)\n",
    "print('Number of days with missing values:', np.size(np.where(nans_per_day_eval > 0)))\n",
    "\n",
    "# Filter out days with no missing values and create a new dataset\n",
    "vectorized_day_dataset_no_nans_eval = vectorized_day_dataset_eval[np.where(nans_per_day_eval == 0)[0], :]\n",
    "days_not_nans_eval = days_eval[np.where(nans_per_day_eval == 0)[0]]\n",
    "\n",
    "# Calculate the final number of days in the evaluation dataset after removing missing values\n",
    "print('Final number of days in evaluation dataset:', len(days_not_nans_eval))\n",
    "\n",
    "# Print the list of days in the evaluation dataset with no missing values\n",
    "print('List of days without missing values:', days_not_nans_eval)\n",
    "\n",
    "# Calculate the total number of days in the filtered evaluation dataset\n",
    "ndays_eval_not_nans = len(days_not_nans_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492167df",
   "metadata": {},
   "source": [
    "### Model evaluation on evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe40a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgglomerativeClustering(metric='euclidean', n_clusters=5)\n",
      "Prediction accuracy MAE: 153.37647598101387\n",
      "Prediction accuracy MAPE: 9.84753902348205\n"
     ]
    }
   ],
   "source": [
    "centroids = []\n",
    "clusters = None\n",
    "n_clusters_t = len(np.unique(cluster_labels))\n",
    "n_clusters = 5\n",
    "\n",
    "#Clustering functions for model evaluation:\n",
    "#clusters = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(vectorized_day_dataset_no_nans) # Parameter tested: {“lloyd”, “elkan”, “auto”, “full”}\n",
    "clusters = AgglomerativeClustering(n_clusters=n_clusters,metric='euclidean', linkage='ward').fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "#clusters = DBSCAN(eps=500, min_samples = 2).fit(vectorized_day_dataset_no_nans) # check the parameters at https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "if clusters is not None:\n",
    "    cluster_labels = clusters.labels_\n",
    "    print(clusters)\n",
    "\n",
    "#cluster_labels = GaussianMixture(n_components=n_clusters, covariance_type=\"tied\").fit(vectorized_day_dataset_no_nans).predict(vectorized_day_dataset_no_nans) #covariance_type{‘full’, ‘tied’, ‘diag’, ‘spherical’}\n",
    "\n",
    "\n",
    "# Define a function to find the closest centroid to a new data point within a specified day-time interval range\n",
    "def find_the_closest_centroid(centroids, new_day, from_interval: int, to_interval: int):\n",
    "    closest_centroid = None\n",
    "    closest_dist = None\n",
    "\n",
    "    # Iterate through each centroid\n",
    "    for i in range(0, len(centroids)):\n",
    "        # Calculate the Euclidean distance between the centroid and the new data point\n",
    "        ed_t = dis_lib.paired_distances(centroids[i], new_day, metric='euclidean')\n",
    "\n",
    "        # Check if the current centroid is closer than the previously closest one\n",
    "        if closest_centroid is None or closest_dist > ed_t:\n",
    "            closest_centroid = i\n",
    "            closest_dist = ed_t\n",
    "\n",
    "    return closest_centroid\n",
    "\n",
    "# Initialize a list to store centroid data\n",
    "centroids = []\n",
    "\n",
    "# Calculate centroids for each cluster\n",
    "for i in range(0, n_clusters_t):\n",
    "    centroid = np.nanmean(vectorized_day_dataset_no_nans[np.where(cluster_labels == i)[0], :], 0).reshape(1, nintvals)\n",
    "    centroids.append(centroid)\n",
    "\n",
    "# Define the number of past intervals to consider for classification\n",
    "n_past_intervals_for_classification = 5\n",
    "\n",
    "# Initialize variables to calculate accuracy metrics\n",
    "total_mae = 0\n",
    "total_mape = 0\n",
    "prediction_counts = 0\n",
    "\n",
    "\n",
    "#print(\"Number of NANs is: \", np.sum(np.isnan(vectorized_day_dataset_no_nans_eval)))\n",
    "\n",
    "\n",
    "# Loop through each day in the evaluation dataset with no missing values\n",
    "for i in range(0, ndays_eval_not_nans):\n",
    "    # Loop through intervals from n_past_intervals_for_classification to nintvals - 1\n",
    "    for j in range(n_past_intervals_for_classification, nintvals - 1):\n",
    "        # Find the closest centroid for the current data point\n",
    "        centroid_index = find_the_closest_centroid(centroids, vectorized_day_dataset_no_nans_eval[i].reshape(1, nintvals), j - n_past_intervals_for_classification, j)\n",
    "\n",
    "        # Predict the value for the next interval\n",
    "        predicted_value = centroids[centroid_index][0, j + 1]\n",
    "\n",
    "        # Calculate Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE)\n",
    "        mae_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
    "        mape_t = abs(predicted_value - vectorized_day_dataset_no_nans_eval[i][j + 1]) / float(vectorized_day_dataset_no_nans_eval[i][j + 1])\n",
    "\n",
    "        # Accumulate MAE, MAPE, and count of predictions\n",
    "        total_mae += mae_t\n",
    "        total_mape += mape_t\n",
    "        prediction_counts += 1\n",
    "\n",
    "# Calculate and print the prediction accuracy metrics\n",
    "print('Prediction accuracy MAE:', total_mae / prediction_counts)\n",
    "print('Prediction accuracy MAPE:', total_mape / prediction_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df01c18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469f32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd82320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
